{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "net_for_hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k0q29Z3ukKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14c9594-f5ea-4a27-eb71-f2c10f7007f5"
      },
      "source": [
        "# delete this cell if working on Pycharm\n",
        "!pip install Bio\n",
        "!pip install import-ipynb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Bio in /usr/local/lib/python3.7/dist-packages (1.3.9)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.7/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.0)\n",
            "Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.7/dist-packages (from Bio) (1.79)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from mygene->Bio) (0.2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCeXnsvlLtE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafe72bb-1147-4fc5-8fc2-6d7f6813ecf5"
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "import matplotlib\n",
        "matplotlib.use('nbagg')\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "# so we can import utils notebook (delete if working on Pycharm), you might need to change it to your working directory path\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "%cd \"drive\"\n",
        "%cd \"MyDrive\"\n",
        "%cd \"Ex4Data\"\n",
        "!ls\n",
        "import import_ipynb\n",
        "import utils_for_hackathon as utils\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/.shortcut-targets-by-id/1o0YBS756Yf3fr4s6rHsprRqjOfdY6PDU/Ex4Data\n",
            "6xw6.fasta\t\tNbTestSet\t\t our_pred.pdb\n",
            "6xw6.pdb\t\tnet_for_hackathon.ipynb  __pycache__\n",
            "first_model.tf\t\tnet.ipynb\t\t second_model.tf\n",
            "fourth_model.tf\t\tour_6xw62.pdb\t\t third_model.tf\n",
            "input_data.npy\t\tour_6xw63.pdb\t\t train_input.npy\n",
            "label_data.npy\t\tour_6xw64.pdb\t\t train_labels.npy\n",
            "model_hackathon.tf\tour_6xw65.pdb\t\t utils_for_hackathon.ipynb\n",
            "model_loss_history.png\tour_6xw6.pdb\t\t utils.ipynb\n",
            "my_model.tf\t\tour_model.tf\n",
            "importing Jupyter notebook from utils_for_hackathon.ipynb\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Bio in /usr/local/lib/python3.7/dist-packages (1.3.9)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.7/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.7/dist-packages (from Bio) (1.79)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from mygene->Bio) (0.2.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y4fRqWLLwhR"
      },
      "source": [
        "###############################################################################\n",
        "#                                                                             #\n",
        "#              Parameters you can change, but don't have to                   #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# number of ResNet blocks for the first ResNet and the kernel size.\n",
        "RESNET_1_BLOCKS = 3 \n",
        "RESNET_1_KERNEL_SIZE = 15\n",
        "RESNET_1_KERNEL_NUM = 64\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                        Parameters you need to choose                        #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# number of ResNet blocks for the second ResNet, dilation list to repeat and the kernel size.\n",
        "\n",
        "RESNET_2_BLOCKS = 3\n",
        "RESNET_2_KERNEL_SIZE = 3\n",
        "RESNET_2_KERNEL_NUM = 32\n",
        "DILATION = [1, 2, 4, 8]\n",
        "\n",
        "# percentage of dropout for the dropout layer\n",
        "DROPOUT = 0.15\n",
        "\n",
        "# number of epochs, Learning rate and Batch size\n",
        "EPOCHS = 60\n",
        "LR = 0.01\n",
        "BATCH = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7wc4YJMo5V-"
      },
      "source": [
        "def resnet_1(input_layer):  # TODO: implement this!\n",
        "    \"\"\"\n",
        "    ResNet layer - input -> BatchNormalization -> Conv1D -> Relu -> BatchNormalization -> Conv1D -> Relu -> Add\n",
        "    :param input_layer: input layer for the ResNet\n",
        "    :return: last layer of the ResNet\n",
        "    \"\"\"\n",
        "    for _ in range(RESNET_1_BLOCKS):\n",
        "      out_batchnorm = layers.BatchNormalization()(input_layer)\n",
        "      out_conv1 = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same', activation='relu')(out_batchnorm)\n",
        "      out_batchnorm = layers.BatchNormalization()(out_conv1)\n",
        "      out_conv1 = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same', activation='relu')(out_batchnorm)\n",
        "      input_layer = layers.Add()([input_layer, out_conv1])\n",
        "    return input_layer\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXj05_d7o5io"
      },
      "source": [
        "def resnet_2(input_layer):  # TODO: implement this!\n",
        "    \"\"\"\n",
        "    Dilated ResNet layer - input -> BatchNormalization -> dilated Conv1D -> Relu -> BatchNormalization -> dilated Conv1D -> Relu -> Add\n",
        "    :param input_layer: input layer for the ResNet\n",
        "    :return: last layer of the ResNet\n",
        "    \"\"\"\n",
        "    for _ in range(RESNET_2_BLOCKS):\n",
        "      for dilation in DILATION:\n",
        "        out_batchnorm = layers.BatchNormalization()(input_layer)\n",
        "        out_conv1 = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, padding='same', activation='relu', dilation_rate=dilation)(out_batchnorm)\n",
        "        out_batchnorm = layers.BatchNormalization()(out_conv1)\n",
        "        out_conv1 = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, padding='same', activation='relu', dilation_rate=dilation)(out_batchnorm)\n",
        "        input_layer = layers.Add()([input_layer, out_conv1])\n",
        "    return input_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CN7eEopE3A"
      },
      "source": [
        "def build_network():\n",
        "    \"\"\"\n",
        "    builds the neural network architecture as shown in the exercise.\n",
        "    :return: a Keras Model\n",
        "    \"\"\"\n",
        "    ...\n",
        "    # input, shape (NB_MAX_LENGTH,FEATURE_NUM)\n",
        "    input_layer = tf.keras.Input(shape=(utils.NB_MAX_LENGTH, utils.FEATURE_NUM))\n",
        "\n",
        "    # Conv1D -> shape = (NB_MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
        "    conv1d_layer = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same')(input_layer)\n",
        "\n",
        "    # first ResNet -> shape = (NB_MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
        "    resnet_layer = resnet_1(conv1d_layer)\n",
        "\n",
        "    # # Conv1D -> shape = (NB_MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
        "    conv1d_layer = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, padding=\"same\")(resnet_layer)\n",
        "\n",
        "    # # second ResNet -> shape = (NB_MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
        "    resnet_layer = resnet_2(conv1d_layer)\n",
        "\n",
        "    dropout_layer = layers.Dropout(DROPOUT)(resnet_layer)\n",
        "\n",
        "    conv1d_layer = layers.Conv1D(np.ceil(RESNET_2_KERNEL_NUM/2), RESNET_2_KERNEL_SIZE, padding=\"same\", activation='elu')(dropout_layer)\n",
        "\n",
        "    output1 = layers.Dense(utils.OUTPUT_SIZE)(conv1d_layer)\n",
        "\n",
        "    output2 = layers.Dense(1)(conv1d_layer)\n",
        "\n",
        "    return tf.keras.Model(input_layer, [output1, output2], name='from_seq_to_struct')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8itDQ59HpFlL"
      },
      "source": [
        "def plot_val_train_loss(history):\n",
        "    \"\"\"\n",
        "    plots the train and validation loss of the model at each epoch, saves it in 'model_loss_history.png'\n",
        "    :param history: history object (output of fit function)\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    ig, axes = plt.subplots(1, 1, figsize=(15,3))\n",
        "    axes.plot(history.history['loss'][10:], label='Training loss')\n",
        "    axes.plot(history.history['val_loss'][10:], label='Validation loss')\n",
        "    axes.legend()\n",
        "    axes.set_title(\"Train and Val MSE loss\")\n",
        "\n",
        "    plt.savefig(\"/content/drive/MyDrive/Ex4Data/model_loss_history\")  # TODO: you can change the path here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_loss(y_true, y_pred):\n",
        "    y_true = y_true[:,:,:-1]\n",
        "    y_pred = y_pred[:,:,:-1]\n",
        "    mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "    return mse(y_true, y_pred)\n",
        "\n",
        "def second_loss(y_true, y_pred):\n",
        "    a = 0.01\n",
        "    acc_pred = tf.squeeze(y_pred[:,:,-1]) # 32 X 140\n",
        "    print(acc_pred.shape)\n",
        "    y_true = y_true[:,:,:-1]\n",
        "    y_pred = y_pred[:,:,:-1]\n",
        "    mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "    acc_val = mse(y_true, y_pred)\n",
        "    return mse(y_true, y_pred) + mse(acc_val, acc_pred)\n",
        "\n",
        "loss_mse_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "def rmsd_loss(y_true, y_pred):\n",
        "  mse_func = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "  mse = mse_func(y_true, y_pred)\n",
        "  rmsd = tf.sqrt(((mse * 15) / 5))\n",
        "  return rmsd \n",
        "\n",
        "\n",
        "def loss_acc_fn(y_true, y_pred, coef):\n",
        "    return coef * rmsd_loss(y_true, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "a-OQjSwVg88U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_network()\n",
        "# Instantiate an optimizer.\n",
        "optimizer= tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "# Instantiate a loss function.\n",
        "\n",
        "loss_fn = first_loss\n",
        "\n",
        "# Prepare the training dataset.\n",
        "# you can load here your input and output data\n",
        "X = np.load('input_data.npy')\n",
        "Y = np.load('label_data.npy')\n",
        "# Y = np.concatenate([Y, np.zeros((Y.shape[0],Y.shape[1],1)).astype('float64') ], -1)\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.batch(BATCH)\n",
        "\n",
        "coef = 0\n",
        "epochs = 60\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    if epoch == 10:\n",
        "      coef = 0.03\n",
        "\n",
        "    print(\"\\nepoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "    loss_value = 0\n",
        "    i = 1\n",
        "    train_mse_losses = []\n",
        "    train_acc_losses = []\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        i = step\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            y_pred, acc_pred = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            rmsd_loss_value = rmsd_loss(y_batch_train, tf.cast(y_pred, tf.float64))\n",
        "            acc_loss_value = loss_acc_fn(mse_loss_value, tf.cast(acc_pred, tf.float64), coef)\n",
        "            \n",
        "            train_mse_losses.append(rmsd_loss_value)\n",
        "            train_acc_losses.append(acc_loss_value)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient([mse_loss_value,acc_loss_value], model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    \n",
        "    ##### end of epoch #######\n",
        "    total_mse_loss = tf.reduce_mean(tf.add_n(train_mse_losses[:-1])) /(i-1)\n",
        "    total_acc_loss = tf.reduce_mean(tf.add_n(train_acc_losses[:-1])) /(i-1)\n",
        "\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    mse_val_losses = []\n",
        "    acc_val_losses = []\n",
        "    j = 1\n",
        "    for k, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
        "        j = k\n",
        "        val_preds, val_acc_pred = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        mse_val_loss_value = loss_mse_fn(y_batch_val, tf.cast(val_preds, tf.float64))\n",
        "        acc_val_loss_value = loss_acc_fn(mse_val_loss_value, tf.cast(val_acc_pred, tf.float64), coef)\n",
        "        mse_val_losses.append(mse_val_loss_value)\n",
        "        acc_val_losses.append(acc_val_loss_value)\n",
        "\n",
        "    total_val_mse_loss = tf.reduce_mean(tf.add_n(mse_val_losses[:-1])) /(j-1)\n",
        "    total_val_acc_loss = tf.reduce_mean(tf.add_n(acc_val_losses[:-1])) /(j-1)\n",
        "\n",
        "    print(\"MSE LOSS: train: %.4f, val: %.4f\" % (float(total_mse_loss,), float(total_val_mse_loss,)))\n",
        "    print(\"ACC LOSS: train: %.4f, val: %.4f\" % (float(total_acc_loss,), float(total_val_acc_loss,)))\n",
        "\n",
        "    if (total_val_mse_loss < 1) and (total_val_acc_loss<0.8) and epoch >11:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntjhXjq_idYs",
        "outputId": "09a7224a-f27a-48d2-eb61-51fa90619986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "MSE LOSS: train: 8.3061, val: 48.1791\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 1\n",
            "MSE LOSS: train: 2.9342, val: 15.3395\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 2\n",
            "MSE LOSS: train: 2.2905, val: 9.4681\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 3\n",
            "MSE LOSS: train: 2.0860, val: 5.3418\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 4\n",
            "MSE LOSS: train: 1.9360, val: 3.2517\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 5\n",
            "MSE LOSS: train: 1.8128, val: 3.6766\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 6\n",
            "MSE LOSS: train: 1.7760, val: 2.0615\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 7\n",
            "MSE LOSS: train: 1.6728, val: 1.8735\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 8\n",
            "MSE LOSS: train: 1.6221, val: 1.3680\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 9\n",
            "MSE LOSS: train: 1.5813, val: 1.3543\n",
            "ACC LOSS: train: 0.0000, val: 0.0000\n",
            "\n",
            "epoch 10\n",
            "MSE LOSS: train: 1.5776, val: 1.3981\n",
            "ACC LOSS: train: 0.2687, val: 0.1849\n",
            "\n",
            "epoch 11\n",
            "MSE LOSS: train: 1.5330, val: 1.3442\n",
            "ACC LOSS: train: 0.0514, val: 0.1677\n",
            "\n",
            "epoch 12\n",
            "MSE LOSS: train: 1.5023, val: 1.2027\n",
            "ACC LOSS: train: 0.0438, val: 0.1717\n",
            "\n",
            "epoch 13\n",
            "MSE LOSS: train: 1.4980, val: 1.1471\n",
            "ACC LOSS: train: 0.0431, val: 0.1588\n",
            "\n",
            "epoch 14\n",
            "MSE LOSS: train: 1.4548, val: 1.1535\n",
            "ACC LOSS: train: 0.0420, val: 0.1565\n",
            "\n",
            "epoch 15\n",
            "MSE LOSS: train: 1.4734, val: 1.2000\n",
            "ACC LOSS: train: 0.0421, val: 0.1667\n",
            "\n",
            "epoch 16\n",
            "MSE LOSS: train: 1.4531, val: 1.1822\n",
            "ACC LOSS: train: 0.0414, val: 0.1584\n",
            "\n",
            "epoch 17\n",
            "MSE LOSS: train: 1.4354, val: 1.2766\n",
            "ACC LOSS: train: 0.0406, val: 0.1581\n",
            "\n",
            "epoch 18\n",
            "MSE LOSS: train: 1.4160, val: 1.0646\n",
            "ACC LOSS: train: 0.0400, val: 0.1576\n",
            "\n",
            "epoch 19\n",
            "MSE LOSS: train: 1.4007, val: 1.0478\n",
            "ACC LOSS: train: 0.0391, val: 0.1509\n",
            "\n",
            "epoch 20\n",
            "MSE LOSS: train: 1.3930, val: 1.0086\n",
            "ACC LOSS: train: 0.0390, val: 0.1524\n",
            "\n",
            "epoch 21\n",
            "MSE LOSS: train: 1.3708, val: 1.0093\n",
            "ACC LOSS: train: 0.0382, val: 0.1538\n",
            "\n",
            "epoch 22\n",
            "MSE LOSS: train: 1.3861, val: 1.1005\n",
            "ACC LOSS: train: 0.0383, val: 0.1506\n",
            "\n",
            "epoch 23\n",
            "MSE LOSS: train: 1.3880, val: 1.0152\n",
            "ACC LOSS: train: 0.0378, val: 0.1547\n",
            "\n",
            "epoch 24\n",
            "MSE LOSS: train: 1.3610, val: 1.0223\n",
            "ACC LOSS: train: 0.0376, val: 0.1500\n",
            "\n",
            "epoch 25\n",
            "MSE LOSS: train: 1.3476, val: 0.9839\n",
            "ACC LOSS: train: 0.0369, val: 0.1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_predict = utils.generate_input(os.path.join(\"/content/drive/MyDrive/Ex4Data\", \"6xw6.pdb\"))\n",
        "prediction = model.predict(to_predict[np.newaxis,:,:])\n",
        "model.save(\"model_hackathon.tf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2h4-m7KvmSq",
        "outputId": "89374d8d-a808-49cd-f41f-c3e271b2a3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: model_hackathon.tf/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-PFY6wavuwk",
        "outputId": "3786c099-b568-4764-a3f0-1a821e0a1b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.9448955 ],\n",
              "        [1.7418382 ],\n",
              "        [1.363431  ],\n",
              "        [1.0181375 ],\n",
              "        [0.9934501 ],\n",
              "        [0.9922468 ],\n",
              "        [1.2001047 ],\n",
              "        [1.2456703 ],\n",
              "        [1.6239715 ],\n",
              "        [1.6880713 ],\n",
              "        [1.8320215 ],\n",
              "        [1.9995065 ],\n",
              "        [1.5750282 ],\n",
              "        [1.6226351 ],\n",
              "        [1.4919109 ],\n",
              "        [1.3211989 ],\n",
              "        [1.3873692 ],\n",
              "        [1.1898785 ],\n",
              "        [1.0811436 ],\n",
              "        [0.9769029 ],\n",
              "        [0.81213206],\n",
              "        [0.9034911 ],\n",
              "        [1.0543442 ],\n",
              "        [1.6187351 ],\n",
              "        [1.8509829 ],\n",
              "        [1.4220653 ],\n",
              "        [1.3730133 ],\n",
              "        [1.3425877 ],\n",
              "        [1.6506405 ],\n",
              "        [1.7240767 ],\n",
              "        [1.3510964 ],\n",
              "        [1.2944655 ],\n",
              "        [0.89131576],\n",
              "        [0.75202096],\n",
              "        [0.5126294 ],\n",
              "        [0.7997649 ],\n",
              "        [0.8475759 ],\n",
              "        [1.2593007 ],\n",
              "        [1.1350553 ],\n",
              "        [1.4036691 ],\n",
              "        [1.7965639 ],\n",
              "        [1.322655  ],\n",
              "        [1.2428031 ],\n",
              "        [1.1876031 ],\n",
              "        [1.1327151 ],\n",
              "        [1.1697977 ],\n",
              "        [1.0557333 ],\n",
              "        [0.9068367 ],\n",
              "        [1.2701629 ],\n",
              "        [1.3647234 ],\n",
              "        [1.8325365 ],\n",
              "        [1.9663737 ],\n",
              "        [2.1027465 ],\n",
              "        [2.113558  ],\n",
              "        [2.1116755 ],\n",
              "        [1.8356807 ],\n",
              "        [1.5521674 ],\n",
              "        [1.4584656 ],\n",
              "        [1.4413238 ],\n",
              "        [1.3733468 ],\n",
              "        [1.59183   ],\n",
              "        [1.3556356 ],\n",
              "        [1.477359  ],\n",
              "        [1.4291122 ],\n",
              "        [1.6805687 ],\n",
              "        [1.1309762 ],\n",
              "        [1.030328  ],\n",
              "        [0.9138561 ],\n",
              "        [0.9118617 ],\n",
              "        [0.9306028 ],\n",
              "        [1.057242  ],\n",
              "        [1.1393209 ],\n",
              "        [1.5960493 ],\n",
              "        [1.675894  ],\n",
              "        [1.8487992 ],\n",
              "        [1.7350109 ],\n",
              "        [1.2023411 ],\n",
              "        [1.200702  ],\n",
              "        [1.1478989 ],\n",
              "        [0.717184  ],\n",
              "        [1.2135835 ],\n",
              "        [0.6859776 ],\n",
              "        [1.0985265 ],\n",
              "        [0.97214323],\n",
              "        [1.1481166 ],\n",
              "        [1.2959343 ],\n",
              "        [1.6127613 ],\n",
              "        [1.5795665 ],\n",
              "        [1.3281729 ],\n",
              "        [1.4945054 ],\n",
              "        [1.1040545 ],\n",
              "        [1.2490189 ],\n",
              "        [0.9108488 ],\n",
              "        [1.0765507 ],\n",
              "        [1.1903827 ],\n",
              "        [1.2726113 ],\n",
              "        [1.5676641 ],\n",
              "        [1.7618263 ],\n",
              "        [2.232005  ],\n",
              "        [2.4800172 ],\n",
              "        [2.6404703 ],\n",
              "        [2.6749337 ],\n",
              "        [2.6077538 ],\n",
              "        [2.600062  ],\n",
              "        [2.702532  ],\n",
              "        [2.3386    ],\n",
              "        [2.3500526 ],\n",
              "        [2.3473058 ],\n",
              "        [2.1600792 ],\n",
              "        [2.270525  ],\n",
              "        [2.2638588 ],\n",
              "        [2.2900476 ],\n",
              "        [2.2248826 ],\n",
              "        [1.94632   ],\n",
              "        [2.109742  ],\n",
              "        [1.8067112 ],\n",
              "        [1.8904018 ],\n",
              "        [1.6096013 ],\n",
              "        [1.7127228 ],\n",
              "        [1.2352768 ],\n",
              "        [1.146791  ],\n",
              "        [0.97174704],\n",
              "        [1.300955  ],\n",
              "        [1.3153917 ],\n",
              "        [1.597148  ],\n",
              "        [1.573797  ],\n",
              "        [0.07886976],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526],\n",
              "        [0.07866526]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pT9amvrSpTrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.05\n",
        "MSE LOSS: train: 0.7906, val: 1.1091\n",
        "ACC LOSS: train: 0.0585, val: 1.5253"
      ],
      "metadata": {
        "id": "qnVNSpqPpTus"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQLxqy33pJk2"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "#     model = build_network()\n",
        "#     print(model.summary())\n",
        "\n",
        "#     # you can load here your input and output data\n",
        "#     X = np.load('input_data.npy')\n",
        "#     Y = np.load('label_data.npy')\n",
        "\n",
        "#     X_train, X_val, y_train, y_val  = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
        "\n",
        "#     my_optimizer= tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "#     model.compile(optimizer=my_optimizer, loss=\"mse\")\n",
        "#     history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH, validation_data=(X_val, y_val))\n",
        "\n",
        "#     model.save(\"our_model.tf\")\n",
        "#     model = tf.keras.models.load_model(\"our_model.tf\")\n",
        "\n",
        "#     # part 3 predict\n",
        "#     to_predict = utils.generate_input(os.path.join(\"/content/drive/MyDrive/Ex4Data\", \"6xw6.pdb\"))\n",
        "#     prediction = model.predict(to_predict[np.newaxis,:,:])\n",
        "#     matrix = utils.matrix_to_pdb(utils.get_seq_aa(os.path.join(\"/content/drive/MyDrive/Ex4Data\", \"6xw6.pdb\"), \"H\")[0], prediction[0, :, :],\"our_6xw6\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"model_hackathon.tf\")\n",
        "to_predict = utils.generate_input(os.path.join(\"/content/drive/MyDrive/Ex4Data/NbTestSet\", \"1fns.pdb\"))\n",
        "struct, acc = model.predict(to_predict[np.newaxis,:,:])\n",
        "matrix = utils.matrix_to_pdb(utils.get_seq_aa(os.path.join(\"/content/drive/MyDrive/Ex4Data/NbTestSet\", \"1fns.pdb\"), \"H\")[0], struct[0, :, :],\"hackathon_result/test_1fns\")\n"
      ],
      "metadata": {
        "id": "SoJbr4YBfdaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797bc4a3-a796-4fc9-e12c-fa4060d76bca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "model = tf.keras.models.load_model(\"our_model.tf\")\n",
        "test_files_path = '/content/drive/MyDrive/Ex4Data/NbTestSet'\n",
        "result_path = '/content/drive/MyDrive/Ex4Data/hackathon_result_ex4_model/'\n",
        "for file in glob.glob(test_files_path + \"/*.pdb\"):\n",
        "    protein_name = os.path.basename(os.path.splitext(file)[0])\n",
        "    to_predict = utils.generate_input(file)\n",
        "    struct = model.predict(to_predict[np.newaxis,:,:])\n",
        "    utils.matrix_to_pdb(utils.get_seq_aa(file, \"H\")[0], struct[0, :, :], result_path + protein_name)\n",
        "    # np.savetxt(result_path + protein_name + '.txt', np.squeeze(acc), fmt='%5f')\n",
        "\n"
      ],
      "metadata": {
        "id": "jkNA6XXGVdsP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nexRMZMRYiaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_path = '/content/drive/MyDrive/Ex4Data/hackathon_result/'\n",
        "\n",
        "np.savetxt(result_path+'tewst_lacc.txt', np.squeeze(acc), fmt='%5f')"
      ],
      "metadata": {
        "id": "_cjFxep1YjN1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def main(args):\n",
        "  pdb_path = args[1]\n",
        "  method = args[2]\n",
        "  run_prediction(pdb_path, method)\n",
        "\n",
        "def run_prediction(pdb_path, method):\n",
        "  if method == \"baseline\":\n",
        "    pass\n",
        "    \n",
        "  if method == \"advanced\":\n",
        "    path_to_advanced_model = \"model_hackathon.tf\"\n",
        "    model = tf.keras.models.load_model(path_to_advanced_model)\n",
        "    protein_name = os.path.basename(os.path.splitext(pdb_path)[0])\n",
        "    to_predict = utils.generate_input(pdb_path)\n",
        "    struct, acc = model.predict(to_predict[np.newaxis,:,:])\n",
        "    utils.matrix_to_pdb(utils.get_seq_aa(pdb_path, \"H\")[0], struct[0, :, :], protein_name)\n",
        "    np.savetxt(protein_name + '_acc.txt', np.squeeze(acc), fmt='%5f')\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(sys.argv)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbDlBVLlZ8T_",
        "outputId": "900e85e0-3074-4e6c-f3fa-89aca25eb6ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.4384, 2.025 , 1.683 , 1.366 , 1.0191, 0.9663, 1.0071, 1.2155,\n",
              "       1.3361, 1.668 , 1.6741, 1.8112, 1.9966, 1.523 , 1.6029, 1.6045,\n",
              "       1.3444, 1.3998, 1.2183, 1.067 , 0.9854, 0.8155, 0.932 , 1.0925,\n",
              "       1.6575, 1.918 , 1.364 , 1.4077, 1.2813, 1.7023, 1.7984, 1.3554,\n",
              "       1.293 , 0.8457, 0.6878, 0.4848, 0.7971, 0.8097, 1.1728, 1.1113,\n",
              "       1.338 , 1.8374, 1.3732, 1.3394, 1.1407, 1.1354, 1.2245, 1.1508,\n",
              "       1.2142, 1.2582, 1.3359, 1.7216, 1.8776, 2.1951, 2.4192, 2.303 ,\n",
              "       2.0503, 1.6735, 1.7282, 1.5102, 1.4842, 1.4883, 1.4509, 1.6203,\n",
              "       1.4305, 1.452 , 1.4408, 1.6735, 1.1348, 1.0274, 0.8618, 0.9096,\n",
              "       0.9396, 1.08  , 1.1387, 1.5991, 1.6655, 1.893 , 1.7262, 1.246 ,\n",
              "       1.2356, 1.1711, 0.7154, 1.2049, 0.6927, 1.0941, 0.9539, 1.1305,\n",
              "       1.2766, 1.6147, 1.5652, 1.345 , 1.5202, 1.109 , 1.2723, 0.9425,\n",
              "       1.0398, 1.1706, 1.2455, 1.5054, 1.7218, 2.1962, 2.4921, 2.5143,\n",
              "       2.3279, 1.9989, 2.1087, 1.7841, 1.8378, 1.635 , 1.725 , 1.2873,\n",
              "       1.1361, 0.9649, 1.3459, 1.3557, 1.6191, 1.6047, 1.8618, 0.0787,\n",
              "       0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787,\n",
              "       0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787, 0.0787,\n",
              "       0.0787, 0.0787, 0.0787, 0.0787], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}